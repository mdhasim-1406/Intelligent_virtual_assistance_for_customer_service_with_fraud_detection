# ğŸ›¡ï¸ SafeServe AI - Intelligent Virtual Assistant for Customer Service with Fraud Detection

[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.1-green.svg)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28.1-red.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

A production-ready AI virtual assistant system that combines real-time fraud detection with natural language customer support, powered by open-source technologies.

## ğŸ¯ Features

- ğŸ¤– **AI-Powered Chatbot**: Deepseek-Coder 6.7B model for intelligent customer service
- ğŸ›¡ï¸ **Real-time Fraud Detection**: ML-based transaction analysis using Isolation Forest
- ğŸ“Š **Interactive Analytics**: Beautiful dashboards with transaction insights
- ğŸŒ **Multi-language Support**: Built-in language switching capability
- ğŸ”§ **Microservices Architecture**: Clean separation of concerns
- ğŸ“± **Responsive UI**: Modern Streamlit interface with custom styling
- ğŸš€ **Production Ready**: Comprehensive error handling and monitoring

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit UI  â”‚â”€â”€â”€â–¶â”‚   FastAPI API   â”‚â”€â”€â”€â–¶â”‚ Fraud Detection â”‚
â”‚     Frontend    â”‚    â”‚     Backend     â”‚    â”‚     Engine      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Deepseek LLM    â”‚
                       â”‚ (Google Colab)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/safeserve-ai.git
cd safeserve-ai
```

### 2. Set Up Python Environment

```bash
# Create virtual environment
python -m venv safeserve-env

# Activate environment
# On Windows:
safeserve-env\Scripts\activate
# On macOS/Linux:
source safeserve-env/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Set Up the LLM Backend (Google Colab)

1. Open Google Colab in your browser
2. Upload and run the notebook: `notebooks/serve_llm.ipynb`
3. Set your ngrok token in the notebook
4. Copy the generated ngrok URL from the notebook output
5. Update the `.env` file with your ngrok URL

### 4. Configure Environment Variables

Edit the `.env` file with your settings:

```bash
# Get your ngrok token from https://ngrok.com/
NGROK_AUTH_TOKEN=your_actual_ngrok_token_here

# This will be generated by the Colab notebook
LLM_API_URL=https://your-ngrok-url.ngrok.io/chat

# API Configuration
API_HOST=0.0.0.0
API_PORT=8080
DEBUG=False
LOG_LEVEL=INFO
```

### 5. Start the Backend API

```bash
# From the root directory
cd backend
python api.py
```

The API will be available at `http://localhost:8080`

### 6. Launch the Frontend

```bash
# From the root directory
cd ui
streamlit run app.py
```

The UI will be available at `http://localhost:8501`

## ğŸ“ Project Structure

```
SafeServe-AI/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api.py                 # FastAPI main application
â”‚   â””â”€â”€ fraud_detection.py     # ML fraud detection engine
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py                 # Streamlit frontend application
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ load_env.py           # Environment configuration loader
â”‚   â””â”€â”€ transaction_loader.py # Transaction data processing
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ serve_llm.ipynb       # Google Colab LLM server
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                # This file
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `NGROK_AUTH_TOKEN` | Your ngrok authentication token | Required |
| `LLM_API_URL` | URL of the LLM API endpoint | `http://localhost:8000/chat` |
| `API_HOST` | Backend API host | `0.0.0.0` |
| `API_PORT` | Backend API port | `8080` |
| `DEBUG` | Enable debug mode | `False` |
| `LOG_LEVEL` | Logging level | `INFO` |

### Fraud Detection Settings

The fraud detection engine uses the following features:
- Transaction amount
- Time of transaction (hour, day of week)
- Location risk score
- Device risk score
- Transaction velocity
- Merchant risk score

## ğŸ› ï¸ API Endpoints

### Fraud Detection
- `POST /predict` - Analyze transaction for fraud
- `GET /transactions` - Get transaction history
- `GET /stats` - Get system statistics

### AI Chat
- `POST /chat` - Chat with AI assistant
- `GET /chats` - Get chat history

### Combined Analysis
- `POST /analyze-transaction-with-chat` - Fraud analysis with AI explanation

### System
- `GET /health` - Health check
- `GET /` - API information

## ğŸ“Š Usage Examples

### Fraud Detection API

```python
import requests

# Analyze a transaction
transaction_data = {
    "amount": 9000,
    "location": "foreign",
    "merchant": "online_shopping",
    "device_id": "unknown",
    "velocity_score": 5.0
}

response = requests.post(
    "http://localhost:8080/predict",
    json=transaction_data
)

result = response.json()
print(f"Risk Score: {result['risk_score']}")
print(f"Label: {result['label']}")
```

### Chat API

```python
import requests

# Chat with AI
chat_request = {
    "query": "What should I do if I suspect fraud?",
    "user_id": "user123"
}

response = requests.post(
    "http://localhost:8080/chat",
    json=chat_request
)

result = response.json()
print(f"AI Response: {result['response']}")
```

## ğŸ§ª Testing

### Test Fraud Detection

```bash
# Test with sample transaction data
cd backend
python fraud_detection.py
```

### Test API Endpoints

```bash
# Install pytest
pip install pytest

# Run tests
pytest tests/
```

## ğŸ” Monitoring & Debugging

### Check System Health

```bash
curl http://localhost:8080/health
```

### View Logs

The application logs are output to the console. Set `LOG_LEVEL=DEBUG` in `.env` for detailed logging.

### API Documentation

Once the backend is running, visit `http://localhost:8080/docs` for interactive API documentation.

## ğŸš¨ Troubleshooting

### Common Issues

**1. LLM API Connection Failed**
- Ensure the Google Colab notebook is running
- Check that ngrok URL is correct in `.env`
- Verify ngrok token is valid

**2. Frontend Can't Connect to Backend**
- Ensure backend API is running on port 8080
- Check firewall settings
- Verify API_HOST and API_PORT in `.env`

**3. Fraud Detection Model Not Training**
- Check if required ML libraries are installed
- Ensure sufficient memory for model training
- Check Python version compatibility (3.8+)

**4. Missing Dependencies**
- Run `pip install -r requirements.txt` again
- Check for version conflicts
- Try creating a fresh virtual environment

## ğŸ“ˆ Performance Optimization

### For Production Deployment

1. **Use a dedicated GPU** for the LLM model
2. **Implement caching** for frequent queries
3. **Use a proper database** instead of in-memory storage
4. **Set up load balancing** for multiple API instances
5. **Implement proper logging** and monitoring

### Scaling Considerations

- **Horizontal scaling**: Deploy multiple API instances
- **Database optimization**: Use PostgreSQL or MongoDB
- **Model optimization**: Use quantized models for faster inference
- **Caching**: Implement Redis for response caching

## ğŸ” Security

### Production Security Checklist

- [ ] Change default API keys and tokens
- [ ] Enable HTTPS for all endpoints
- [ ] Implement rate limiting
- [ ] Add authentication and authorization
- [ ] Sanitize all user inputs
- [ ] Use secrets management system
- [ ] Enable CORS properly
- [ ] Implement request validation

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Deepseek AI** for the open-source language model
- **FastAPI** for the excellent API framework
- **Streamlit** for the beautiful UI framework
- **scikit-learn** and **PyOD** for machine learning capabilities
- **ngrok** for tunneling solutions

## ğŸ“ Support

For support, please open an issue on GitHub or contact the maintainers.

---

**âš¡ Built with â¤ï¸ for the open-source community**

*SafeServe AI - Protecting your transactions with artificial intelligence*
